{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle \n",
    "import re\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from utility import Stemmizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_stops():\n",
    "    res = set()\n",
    "    with open('stops.txt') as in_file:\n",
    "        for line in in_file:\n",
    "            res.add(line.strip())\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = Stemmizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polit_views = {\n",
    "                1 : 'Communist',\n",
    "                2 : 'Socialist',\n",
    "                3 : 'Moderate',\n",
    "                4 : 'Liberal',\n",
    "                5 : 'Conservative',\n",
    "                6 : 'Monarchist',\n",
    "                7 : 'Ultraconservative',\n",
    "                8 : 'Apathetic',\n",
    "                9 : 'Libertian'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_ids_dict(db):\n",
    "    if os.path.exists('ids_indices_dict.pickle'):\n",
    "        with open('ids_indices_dict.pickle', 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "    ids = [user['uid'] for user in db.users.find()]\n",
    "    indices = list(range(len(ids)))\n",
    "\n",
    "    dictionary = dict(zip(ids, indices))\n",
    "    dictionary.update(zip(indices, ids))\n",
    "\n",
    "    with open('ids_indices_dict.pickle', 'wb') as handle:\n",
    "        pickle.dump(dictionary, handle)\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def create_or_load_users_to_posts(db, ids_indices_dict):\n",
    "    if os.path.exists('users_to_posts.pickle'):\n",
    "        with open('users_to_posts.pickle', 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "    users_to_posts = defaultdict(str)\n",
    "\n",
    "    for user_posts in tqdm.tqdm_notebook(db.wall_posts.find(), total=db.wall_posts.count()):\n",
    "        for p in user_posts['posts']:\n",
    "            users_to_posts[ids_indices_dict[user_posts['uid']]] += p['text']\n",
    "\n",
    "    with open('users_to_posts.pickle', 'wb') as handle:\n",
    "        pickle.dump(users_to_posts, handle)\n",
    "\n",
    "    return users_to_posts\n",
    "\n",
    "\n",
    "def stemming(text):    \n",
    "    return s.process(text)\n",
    "\n",
    "def links_content_map():\n",
    "    res = defaultdict(list)\n",
    "    \n",
    "    for link in db.links_content.find():\n",
    "        res[link['url']].append(link)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def user_info_map():\n",
    "    res = dict()\n",
    "    \n",
    "    for u in db.user_info.find():\n",
    "        res[u['uid']] = u\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def user_links_map():\n",
    "    res = defaultdict(list)\n",
    "    \n",
    "    for user_links in db.links.find():\n",
    "        res[user_links['uid']].append(user_links)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def process_user(db, uid, users_to_posts, ids_indices_dict, links_content, user_links_m, user_info):\n",
    "    text = ''\n",
    "    \n",
    "    def not_none(value):\n",
    "        return value if value is not None else ''\n",
    "\n",
    "    for user_links in user_links_m[uid]:\n",
    "        for user_link in user_links['links']:\n",
    "            for link in links_content[user_link]:\n",
    "                if link['type'] == 'sprashivai':\n",
    "                    text += ' '.join(not_none(link['answers']))\n",
    "                elif link['type'] == 'livejournal' or link['type'] == 'pikabu':\n",
    "                    text += ' ' + not_none(link['title'])\n",
    "                    text += ' ' + not_none(link['text'])\n",
    "                elif link['type'] == 'youtube':\n",
    "                    text += ' ' + not_none(link['description'])\n",
    "                    text += ' '.join(not_none(link['tags']))\n",
    "                    text += ' ' + not_none(link['name'])\n",
    "                elif link['type'] == 'ali':\n",
    "                    text += ' ' + not_none(link['name'])\n",
    "                elif link['type'] == 'ask':\n",
    "                    text += ' '.join(not_none(link['answers']))\n",
    "                elif link['type'] == 'unknown':\n",
    "                    text += ' ' + not_none(link['description'])\n",
    "                    text += ' ' + not_none(link['title'])\n",
    "    \n",
    "    if uid in user_info:\n",
    "        u = defaultdict(str, user_info[uid])\n",
    "\n",
    "        text += ' ' + u['about']\n",
    "        text += ' ' + u['quotes']\n",
    "        text += ' ' + u['activities']\n",
    "        text += ' ' + u['interests']\n",
    "        text += ' ' + u['music']\n",
    "        text += ' ' + u['movies']\n",
    "        text += ' ' + u['tv']\n",
    "        text += ' ' + u['books']\n",
    "\n",
    "        text += ' ' + users_to_posts[ids_indices_dict[uid]]\n",
    "    \n",
    "    return stemming(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.ir_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user_info = user_info_map()\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_or_create_texts():\n",
    "    try:\n",
    "        with open('users_texts.bin', 'rb') as in_file:\n",
    "            return pickle.load(in_file)\n",
    "    except:\n",
    "        ids = create_or_load_ids_dict(db)\n",
    "        users_to_posts = create_or_load_users_to_posts(db, ids)\n",
    "        user_info = user_info_map()\n",
    "        \n",
    "        links_content = links_content_map()\n",
    "        user_links = user_links_map()\n",
    "        \n",
    "        print('Total links parsed: {}'.format(len(links_content)))\n",
    "        \n",
    "        users_texts = dict()\n",
    "        total=db.users.count()\n",
    "        \n",
    "        timer_step = 10\n",
    "        times = list()\n",
    "        \n",
    "        for i, u in enumerate(db.users.find(no_cursor_timeout=True)):\n",
    "            start = time.time()\n",
    "            users_texts[u['uid']] = process_user(db, u['uid'], users_to_posts, ids, links_content, user_links, user_info)\n",
    "            end = time.time()\n",
    "            func_time = end - start\n",
    "            times.append(func_time)\n",
    "            if i % 10 == 0:\n",
    "                eta = (total - i + 1)/timer_step*np.mean(times)\n",
    "                print('Processed: {}/{}, ETA: {:.3f}m, time per {} iterations: {:.3f}'\n",
    "                      .format(i, total, eta/60, timer_step, np.mean(times)), end='\\r')\n",
    "                \n",
    "        with open('users_texts.bin', 'wb') as out:\n",
    "            pickle.dump(users_texts, out)\n",
    "            \n",
    "        return users_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links parsed: 348195\n",
      "Processed: 1160/707961, ETA: 141.558m, time per 10 iterations: 0.120\r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "184828225",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-00075606d5f8>\u001b[0m in \u001b[0;36mread_or_create_texts\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'users_texts.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0min_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'users_texts.bin'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e0337e61ba7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0musers_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_or_create_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-00075606d5f8>\u001b[0m in \u001b[0;36mread_or_create_texts\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_cursor_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0musers_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers_to_posts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinks_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_links\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mfunc_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0e1936ee5f63>\u001b[0m in \u001b[0;36mprocess_user\u001b[0;34m(db, uid, users_to_posts, ids_indices_dict, links_content, user_links_m, user_info)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnot_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'about'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 184828225"
     ]
    }
   ],
   "source": [
    "users_texts = read_or_create_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(users_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_texts = {uid:text for uid, text in users_texts.items() if len(text) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_texts = [t for t in users_texts.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOP_WORDS = 10\n",
    "TOPICS_COUNT = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(min_df = 7, max_df = 0.9, stop_words=stops)\n",
    "tf = tf_vectorizer.fit_transform(only_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=TOPICS_COUNT, max_iter=40,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0, verbose=1, n_jobs=1).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "репост сделать запись конкурс группа победитель место друг розыгрыш свой\n",
      "Topic #1:\n",
      "50рубль рубль 40рубль 100рубль 300рубль 200рубль 30рубль 120рубль 250рубль 10рубль\n",
      "Topic #2:\n",
      "дом автомобиль квартира ремонт машина метр комната дверь система материал\n",
      "Topic #3:\n",
      "любить свой друг хороший самый любовь знать женщина мужчина большой\n",
      "Topic #4:\n",
      "святой секс украина господь бог молиться ангел ти мина ми\n",
      "Topic #5:\n",
      "игра gt получить уровень набрать бонус заходить открытка очки новый\n",
      "Topic #6:\n",
      "аниме дрова сорняк пётр story двор pikabu идти кун маркер\n",
      "Topic #7:\n",
      "мир русский война россия стать свой страна история земля век\n",
      "Topic #8:\n",
      "фильм жанр сша история хороший дом любовь самый комедия реж\n",
      "Topic #9:\n",
      "рубль цена купить магазин cc заказ товар скидка доставка сайт\n",
      "Topic #10:\n",
      "ряд плата лицо петлить сантиметр вязать петля лицевой спица сторона\n",
      "Topic #11:\n",
      "язык слово английский буква урок ctrl страница текст alt извинить\n",
      "Topic #12:\n",
      "работа рубль свой компания сайт проект работать россия деньга бизнес\n",
      "Topic #13:\n",
      "масло литр соль добавить яйцо минута ст ингредиент приготовление перец\n",
      "Topic #14:\n",
      "цвета красный цветок цвет зелёный чёрный белый жёлтый роза новый\n",
      "Topic #15:\n",
      "свой самый должный делать большой ребёнок хороший сделать дело отношение\n",
      "Topic #16:\n",
      "музей петербург место адрес город улица ул дом проспект далее\n",
      "Topic #17:\n",
      "fuck маша аромат родинка visitsuomi парфюм месси парфюмерный van окталия\n",
      "Topic #18:\n",
      "москва билет концерт клуб лето любимый праздник питер хороший ждать\n",
      "Topic #19:\n",
      "youtube видео музыка watch альбом канал смотреть music list песня\n",
      "Topic #20:\n",
      "нe нa чтo этo кaк тo мeнить пo oт любить\n",
      "Topic #21:\n",
      "книга роман учебник рассказ александр писатель андрей мир де михаил\n",
      "Topic #22:\n",
      "co love moscow life day one like go time fashion\n",
      "Topic #23:\n",
      "вода кожа нога волос рука растение средство организм упражнение хороший\n",
      "Topic #24:\n",
      "ребёнок собака дом помочь помощь мама малыш просить найти кошка\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, TOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('vect.bin', 'wb') as out:\n",
    "    pickle.dump(tf_vectorizer, out)\n",
    "with open('lda.bin', 'wb') as out:\n",
    "    pickle.dump(lda, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('vect.bin', 'rb') as in_file:\n",
    "    vect = pickle.load(in_file)\n",
    "with open('lda.bin', 'rb') as in_file:\n",
    "    lda = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = vect.transform(only_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_matrix = lda.transform(topics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f9596b7c948>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.topics.insert_many([{'uid': uid, 'topics':t.tolist()} for t, uid in zip(topics_matrix, users_texts.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
