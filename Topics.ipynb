{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle \n",
    "import re\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "try:\n",
    "    from pymongo import MongoClient\n",
    "except:\n",
    "    # there is no mongo on  AU server\n",
    "    pass\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from utility import Stemmizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_stops():\n",
    "    res = set()\n",
    "    with open('stops.txt') as in_file:\n",
    "        for line in in_file:\n",
    "            res.add(line.strip())\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# morpher = MorphAnalyzer()\n",
    "\n",
    "# russian_stopwords = set(stopwords.words('russian'))\n",
    "# english_stopwords = set(stopwords.words('english'))\n",
    "# custom_stops = {'br', 'ask', 'fm', 'http', 'https', 'www', 'ru', 'com', 'vk', 'view',\n",
    "#                 'vkontakte', 'd1', 'd0', 'amp', 'utm_source',  'utm_medium', 'utm_campaign'}\n",
    "\n",
    "# stops = russian_stopwords | english_stopwords | custom_stops | read_stops()\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "s = Stemmizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polit_views = {\n",
    "                1 : 'Communist',\n",
    "                2 : 'Socialist',\n",
    "                3 : 'Moderate',\n",
    "                4 : 'Liberal',\n",
    "                5 : 'Conservative',\n",
    "                6 : 'Monarchist',\n",
    "                7 : 'Ultraconservative',\n",
    "                8 : 'Apathetic',\n",
    "                9 : 'Libertian'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_or_load_ids_dict(db):\n",
    "    if os.path.exists('ids_indices_dict.pickle'):\n",
    "        with open('ids_indices_dict.pickle', 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "    ids = [user['uid'] for user in db.users.find()]\n",
    "    indices = list(range(len(ids)))\n",
    "\n",
    "    dictionary = dict(zip(ids, indices))\n",
    "    dictionary.update(zip(indices, ids))\n",
    "\n",
    "    with open('ids_indices_dict.pickle', 'wb') as handle:\n",
    "        pickle.dump(dictionary, handle)\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def create_or_load_users_to_posts(db, ids_indices_dict):\n",
    "    if os.path.exists('users_to_posts.pickle'):\n",
    "        with open('users_to_posts.pickle', 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "    users_to_posts = defaultdict(str)\n",
    "\n",
    "    for user_post in tqdm.tqdm_notebook(db.wall_posts.find(), total=db.wall_posts.count()):\n",
    "        users_to_posts[ids_indices_dict[user_post['from_id']]] += user_post['text']\n",
    "\n",
    "    with open('users_to_posts.pickle', 'wb') as handle:\n",
    "        pickle.dump(users_to_posts, handle)\n",
    "\n",
    "    return users_to_posts\n",
    "\n",
    "\n",
    "def stemming(text):    \n",
    "    return s.process(text)\n",
    "\n",
    "def links_content_map():\n",
    "    res = defaultdict(list)\n",
    "    \n",
    "    for link in db.links_content.find():\n",
    "        res[link['url']].append(link)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def user_links_map():\n",
    "    res = defaultdict(list)\n",
    "    \n",
    "    for user_links in db.links.find():\n",
    "        res[user_links['uid']].append(user_links)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def process_user(db, uid, users_to_posts, ids_indices_dict, links_content, user_links_m):\n",
    "    text = ''\n",
    "\n",
    "    def not_none(value):\n",
    "        return value if value is not None else ''\n",
    "\n",
    "    for user_links in user_links_m[uid]:\n",
    "        for user_link in user_links['links']:\n",
    "            for link in links_content[user_link]:\n",
    "                if link['type'] == 'sprashivai':\n",
    "                    text += ' '.join(not_none(link['answers']))\n",
    "                elif link['type'] == 'livejournal' or link['type'] == 'pikabu':\n",
    "                    text += ' ' + not_none(link['title'])\n",
    "                    text += ' ' + not_none(link['text'])\n",
    "                elif link['type'] == 'youtube':\n",
    "                    text += ' ' + not_none(link['description'])\n",
    "                    text += ' '.join(not_none(link['tags']))\n",
    "                    text += ' ' + not_none(link['name'])\n",
    "                elif link['type'] == 'ali':\n",
    "                    text += ' ' + not_none(link['name'])\n",
    "                elif link['type'] == 'ask':\n",
    "                    text += ' '.join(not_none(link['answers']))\n",
    "                elif link['type'] == 'unknown':\n",
    "                    text += ' ' + not_none(link['description'])\n",
    "                    text += ' ' + not_none(link['title'])\n",
    "                    \n",
    "    for u in db.user_info.find({'uid':uid}):\n",
    "        u =  defaultdict(str, u)\n",
    "        \n",
    "        text += ' ' + u['about']\n",
    "        text += ' ' + u['quotes']\n",
    "        text += ' ' + u['activities']\n",
    "        text += ' ' + u['interests']\n",
    "        text += ' ' + u['music']\n",
    "        text += ' ' + u['movies']\n",
    "        text += ' ' + u['tv']\n",
    "        text += ' ' + u['books']\n",
    "        \n",
    "    text += ' ' + users_to_posts[ids_indices_dict[uid]]\n",
    "    \n",
    "    return stemming(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.ir_project_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_or_create_texts():\n",
    "    try:\n",
    "        with open('users_texts.bin', 'rb') as in_file:\n",
    "            return pickle.load(in_file)\n",
    "    except:\n",
    "        ids = create_or_load_ids_dict(db)\n",
    "        users_to_posts = create_or_load_users_to_posts(db, ids)\n",
    "        \n",
    "        links_content = links_content_map()\n",
    "        user_links = user_links_map()\n",
    "        \n",
    "        print('Total links parsed: {}'.format(len(links_content)))\n",
    "        \n",
    "        users_texts = dict()\n",
    "\n",
    "        for u in tqdm.tqdm_notebook(db.users.find(no_cursor_timeout=True), total=db.users.count()):\n",
    "            users_texts[u['uid']] = process_user(db, u['uid'], users_to_posts, ids, links_content, user_links)\n",
    "            \n",
    "        with open('users_texts.bin', 'wb') as out:\n",
    "            pickle.dump(users_texts, out)\n",
    "            \n",
    "        return users_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links parsed: 62124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cd9f2ac8fa4b8295438a797231dfbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users_texts = read_or_create_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_texts = {uid:text for uid, text in users_texts.items() if len(text) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_texts = [t for t in users_texts.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOP_WORDS = 10\n",
    "TOPICS_COUNT = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(min_df = 7, max_df = 0.9, stop_words=stops)\n",
    "tf = tf_vectorizer.fit_transform(only_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=TOPICS_COUNT, max_iter=40,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0, verbose=1, n_jobs=1).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "репост сделать запись конкурс группа победитель место друг розыгрыш свой\n",
      "Topic #1:\n",
      "50рубль рубль 40рубль 100рубль 300рубль 200рубль 30рубль 120рубль 250рубль 10рубль\n",
      "Topic #2:\n",
      "дом автомобиль квартира ремонт машина метр комната дверь система материал\n",
      "Topic #3:\n",
      "любить свой друг хороший самый любовь знать женщина мужчина большой\n",
      "Topic #4:\n",
      "святой секс украина господь бог молиться ангел ти мина ми\n",
      "Topic #5:\n",
      "игра gt получить уровень набрать бонус заходить открытка очки новый\n",
      "Topic #6:\n",
      "аниме дрова сорняк пётр story двор pikabu идти кун маркер\n",
      "Topic #7:\n",
      "мир русский война россия стать свой страна история земля век\n",
      "Topic #8:\n",
      "фильм жанр сша история хороший дом любовь самый комедия реж\n",
      "Topic #9:\n",
      "рубль цена купить магазин cc заказ товар скидка доставка сайт\n",
      "Topic #10:\n",
      "ряд плата лицо петлить сантиметр вязать петля лицевой спица сторона\n",
      "Topic #11:\n",
      "язык слово английский буква урок ctrl страница текст alt извинить\n",
      "Topic #12:\n",
      "работа рубль свой компания сайт проект работать россия деньга бизнес\n",
      "Topic #13:\n",
      "масло литр соль добавить яйцо минута ст ингредиент приготовление перец\n",
      "Topic #14:\n",
      "цвета красный цветок цвет зелёный чёрный белый жёлтый роза новый\n",
      "Topic #15:\n",
      "свой самый должный делать большой ребёнок хороший сделать дело отношение\n",
      "Topic #16:\n",
      "музей петербург место адрес город улица ул дом проспект далее\n",
      "Topic #17:\n",
      "fuck маша аромат родинка visitsuomi парфюм месси парфюмерный van окталия\n",
      "Topic #18:\n",
      "москва билет концерт клуб лето любимый праздник питер хороший ждать\n",
      "Topic #19:\n",
      "youtube видео музыка watch альбом канал смотреть music list песня\n",
      "Topic #20:\n",
      "нe нa чтo этo кaк тo мeнить пo oт любить\n",
      "Topic #21:\n",
      "книга роман учебник рассказ александр писатель андрей мир де михаил\n",
      "Topic #22:\n",
      "co love moscow life day one like go time fashion\n",
      "Topic #23:\n",
      "вода кожа нога волос рука растение средство организм упражнение хороший\n",
      "Topic #24:\n",
      "ребёнок собака дом помочь помощь мама малыш просить найти кошка\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, TOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('vect.bin', 'wb') as out:\n",
    "    pickle.dump(tf_vectorizer, out)\n",
    "with open('lda.bin', 'wb') as out:\n",
    "    pickle.dump(lda, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('vect.bin', 'rb') as in_file:\n",
    "    vect = pickle.load(in_file)\n",
    "with open('lda.bin', 'rb') as in_file:\n",
    "    lda = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = vect.transform(only_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_matrix = lda.transform(topics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f9596b7c948>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.topics.insert_many([{'uid': uid, 'topics':t.tolist()} for t, uid in zip(topics_matrix, users_texts.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
