{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle \n",
    "import re\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_stops():\n",
    "    res = set()\n",
    "    with open('stops.txt') as in_file:\n",
    "        for line in in_file:\n",
    "            res.add(line.strip())\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpher = MorphAnalyzer()\n",
    "\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "custom_stops = {'br', 'ask', 'fm', 'http', 'https', 'www', 'ru', 'com', 'vk', 'view'}\n",
    "\n",
    "stops = russian_stopwords | english_stopwords | custom_stops | read_stops()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polit_views = {\n",
    "                1 : 'Communist',\n",
    "                2 : 'Socialist',\n",
    "                3 : 'Moderate',\n",
    "                4 : 'Liberal',\n",
    "                5 : 'Conservative',\n",
    "                6 : 'Monarchist',\n",
    "                7 : 'Ultraconservative',\n",
    "                8 : 'Apathetic',\n",
    "                9 : 'Libertian'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_or_load_ids_dict(db):\n",
    "    if os.path.exists('ids_indices_dict.pickle'):\n",
    "        with open('ids_indices_dict.pickle', 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "    ids = [user['uid'] for user in db.users.find()]\n",
    "    indices = list(range(len(ids)))\n",
    "\n",
    "    dictionary = dict(zip(ids, indices))\n",
    "    dictionary.update(zip(indices, ids))\n",
    "\n",
    "    with open('ids_indices_dict.pickle', 'wb') as handle:\n",
    "        pickle.dump(dictionary, handle)\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def create_or_load_users_to_posts(db, ids_indices_dict):\n",
    "    if os.path.exists('users_to_posts.pickle'):\n",
    "        with open('users_to_posts.pickle', 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "    users_to_posts = defaultdict(str)\n",
    "\n",
    "    for user_post in tqdm.tqdm_notebook(db.wall_posts.find(), total=db.wall_posts.count()):\n",
    "        users_to_posts[ids_indices_dict[user_post['from_id']]] += user_post['text']\n",
    "\n",
    "    with open('users_to_posts.pickle', 'wb') as handle:\n",
    "        pickle.dump(users_to_posts, handle)\n",
    "\n",
    "    return users_to_posts\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def stemming(text):\n",
    "    def is_valid(w):\n",
    "        return not(w.startswith('id') or is_number(w) or w in stops)\n",
    "    \n",
    "    words = [w.lower() for w in tokenizer.tokenize(text)]\n",
    "    words = [morpher.parse(word)[0].normal_form for word in words if is_valid(word)]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "def process_user(db, uid, users_to_posts, ids_indices_dict):\n",
    "    text = ''\n",
    "\n",
    "    def not_none(value):\n",
    "        return value if value is not None else ''\n",
    "\n",
    "    for user_links in db.links.find({'uid': uid}):\n",
    "        for user_link in user_links['links']:\n",
    "            for link in db.links_content.find({'url': user_link}):\n",
    "                if link['type'] == 'sprashivai':\n",
    "                    text += ' '.join(not_none(link['answers']))\n",
    "                elif link['type'] == 'livejournal' or link['type'] == 'pikabu':\n",
    "                    text += ' ' + not_none(link['title'])\n",
    "                    text += ' ' + not_none(link['text'])\n",
    "                elif link['type'] == 'youtube':\n",
    "                    text += ' ' + not_none(link['description'])\n",
    "                    text += ' '.join(not_none(link['tags']))\n",
    "                    text += ' ' + not_none(link['name'])\n",
    "                elif link['type'] == 'ali':\n",
    "                    text += ' ' + not_none(link['name'])\n",
    "                elif link['type'] == 'ask':\n",
    "                    text += ' '.join(not_none(link['answers']))\n",
    "                elif link['type'] == 'unknown':\n",
    "                    text += ' ' + not_none(link['description'])\n",
    "                    text += ' ' + not_none(link['title'])\n",
    "                    \n",
    "    for u in db.user_info.find({'uid':uid}):\n",
    "        u =  defaultdict(str, u)\n",
    "        \n",
    "        text += ' ' + u['about']\n",
    "        text += ' ' + u['quotes']\n",
    "        text += ' ' + u['activities']\n",
    "        text += ' ' + u['interests']\n",
    "        text += ' ' + u['music']\n",
    "        text += ' ' + u['movies']\n",
    "        text += ' ' + u['tv']\n",
    "        text += ' ' + u['books']\n",
    "        \n",
    "    text += ' ' + users_to_posts[ids_indices_dict[uid]]\n",
    "    \n",
    "    return stemming(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# client = MongoClient()\n",
    "# db = client.ir_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_or_create_texts():\n",
    "    try:\n",
    "        with open('users_texts.bin', 'rb') as in_file:\n",
    "            return pickle.load(in_file)\n",
    "    except:\n",
    "        ids = create_or_load_ids_dict(db)\n",
    "        users_to_posts = create_or_load_users_to_posts(db, ids)\n",
    "        \n",
    "        \n",
    "        users_texts = dict()\n",
    "        for uid in tqdm.tqdm_notebook(ids.keys(), total=db.users.count()):\n",
    "            users_texts[uid] = process_user(db, uid, users_to_posts, ids)\n",
    "        with open('users_texts.bin', 'wb') as out:\n",
    "            pickle.dump(users_texts, out)\n",
    "            \n",
    "        return users_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_texts = read_or_create_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_texts = {uid:text for uid, text in users_texts.items() if len(text) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_texts = [t for t in users_texts.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOP_WORDS = 10\n",
    "TOPICS_COUNT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(min_df = 7, max_df = 0.9, stop_words=stops)\n",
    "tf = tf_vectorizer.fit_transform(only_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44074, 104334)\n"
     ]
    }
   ],
   "source": [
    "print(tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmyrov/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=TOPICS_COUNT, max_iter=10,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0, verbose=1, n_jobs=10).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "заклинание орхидея раб магия боязнь противник гарри тысяча порча проклятие\n",
      "Topic #1:\n",
      "репост запись конкурс группа наш сделать весь победитель место розыгрыш\n",
      "Topic #2:\n",
      "d0 amp альбом co d1 музыка music песня английский love\n",
      "Topic #3:\n",
      "любить секс украина club59394712 ти го мина ми мена що\n",
      "Topic #4:\n",
      "ряд плата лицо amp петля сбн utm_source utm_medium utm_campaign social\n",
      "Topic #5:\n",
      "весь это человек свой жизнь любить который хотеть мочь друг\n",
      "Topic #6:\n",
      "год весь это который день работа рубль наш свой россия\n",
      "Topic #7:\n",
      "игра получить уровень amp app3882511 набрать бонус очки заходить ad_id\n",
      "Topic #8:\n",
      "спасибо весь день свадьба москва лето фото instagram любимый очень\n",
      "Topic #9:\n",
      "ребёнок весь год это очень мочь дом помочь помощь собака\n",
      "Topic #10:\n",
      "это нога рука растение упражнение который год весь вода каждый\n",
      "Topic #11:\n",
      "это свой который человек мочь весь ваш жизнь время самый\n",
      "Topic #12:\n",
      "gt открытка свой отправить друг фото сделать смотреть новый vkontakte\n",
      "Topic #13:\n",
      "масло год литр соль минута добавить яйцо ст ложка ингредиент\n",
      "Topic #14:\n",
      "музей петербург адрес ул улица место проспект далее парк пр\n",
      "Topic #15:\n",
      "50рубль рубль 40рубль 100рубль 300рубль 200рубль 30рубль 120рубль 250рубль 10рубль\n",
      "Topic #16:\n",
      "фильм год книга история который жанр мир сша жизнь человек\n",
      "Topic #17:\n",
      "нe __ ___ ____ нa чтo ______ _____ любить ________\n",
      "Topic #18:\n",
      "кожа день вода волос средство организм мёд маска лицо витамин\n",
      "Topic #19:\n",
      "youtube watch александр list николай андрей россия сергей татьяна елена\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, TOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
