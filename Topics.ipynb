{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle \n",
    "import re\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "try:\n",
    "    from pymongo import MongoClient\n",
    "except:\n",
    "    # there is no mongo on  AU server\n",
    "    pass\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stops():\n",
    "    res = set()\n",
    "    with open('stops.txt') as in_file:\n",
    "        for line in in_file:\n",
    "            res.add(line.strip())\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpher = MorphAnalyzer()\n",
    "\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "custom_stops = {'br', 'ask', 'fm', 'http', 'https', 'www', 'ru', 'com', 'vk', 'view',\n",
    "                'vkontakte', 'd1', 'd0', 'amp', 'utm_source',  'utm_medium', 'utm_campaign'}\n",
    "\n",
    "stops = russian_stopwords | english_stopwords | custom_stops | read_stops()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "polit_views = {\n",
    "                1 : 'Communist',\n",
    "                2 : 'Socialist',\n",
    "                3 : 'Moderate',\n",
    "                4 : 'Liberal',\n",
    "                5 : 'Conservative',\n",
    "                6 : 'Monarchist',\n",
    "                7 : 'Ultraconservative',\n",
    "                8 : 'Apathetic',\n",
    "                9 : 'Libertian'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_ids_dict(db):\n",
    "    if os.path.exists('ids_indices_dict.pickle'):\n",
    "        with open('ids_indices_dict.pickle', 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "    ids = [user['uid'] for user in db.users.find()]\n",
    "    indices = list(range(len(ids)))\n",
    "\n",
    "    dictionary = dict(zip(ids, indices))\n",
    "    dictionary.update(zip(indices, ids))\n",
    "\n",
    "    with open('ids_indices_dict.pickle', 'wb') as handle:\n",
    "        pickle.dump(dictionary, handle)\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def create_or_load_users_to_posts(db, ids_indices_dict):\n",
    "    if os.path.exists('users_to_posts.pickle'):\n",
    "        with open('users_to_posts.pickle', 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "\n",
    "    users_to_posts = defaultdict(str)\n",
    "\n",
    "    for user_post in tqdm.tqdm_notebook(db.wall_posts.find(), total=db.wall_posts.count()):\n",
    "        users_to_posts[ids_indices_dict[user_post['from_id']]] += user_post['text']\n",
    "\n",
    "    with open('users_to_posts.pickle', 'wb') as handle:\n",
    "        pickle.dump(users_to_posts, handle)\n",
    "\n",
    "    return users_to_posts\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_valid(w):\n",
    "    return not(w.startswith('id') or is_number(w) or w in stops \n",
    "               or w.startswith('club') or w.startswith('app') or set(w) == {'_'})\n",
    "    \n",
    "def stemming(text):    \n",
    "    words = [morpher.parse(w.lower())[0].normal_form for w in tokenizer.tokenize(text)]\n",
    "    words = [word for word in words if is_valid(word)]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "def links_content_map():\n",
    "    res = defaultdict(list)\n",
    "    \n",
    "    for link in db.links_content.find():\n",
    "        res[link['url']].append(link)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def user_links_map():\n",
    "    res = defaultdict(list)\n",
    "    \n",
    "    for user_links in db.links.find():\n",
    "        res[user_links['uid']].append(user_links)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def process_user(db, uid, users_to_posts, ids_indices_dict, links_content, user_links_m):\n",
    "    text = ''\n",
    "\n",
    "    def not_none(value):\n",
    "        return value if value is not None else ''\n",
    "\n",
    "    for user_links in user_links_m[uid]:\n",
    "        for user_link in user_links['links']:\n",
    "            for link in links_content[user_link]:\n",
    "                if link['type'] == 'sprashivai':\n",
    "                    text += ' '.join(not_none(link['answers']))\n",
    "                elif link['type'] == 'livejournal' or link['type'] == 'pikabu':\n",
    "                    text += ' ' + not_none(link['title'])\n",
    "                    text += ' ' + not_none(link['text'])\n",
    "                elif link['type'] == 'youtube':\n",
    "                    text += ' ' + not_none(link['description'])\n",
    "                    text += ' '.join(not_none(link['tags']))\n",
    "                    text += ' ' + not_none(link['name'])\n",
    "                elif link['type'] == 'ali':\n",
    "                    text += ' ' + not_none(link['name'])\n",
    "                elif link['type'] == 'ask':\n",
    "                    text += ' '.join(not_none(link['answers']))\n",
    "                elif link['type'] == 'unknown':\n",
    "                    text += ' ' + not_none(link['description'])\n",
    "                    text += ' ' + not_none(link['title'])\n",
    "                    \n",
    "    for u in db.user_info.find({'uid':uid}):\n",
    "        u =  defaultdict(str, u)\n",
    "        \n",
    "        text += ' ' + u['about']\n",
    "        text += ' ' + u['quotes']\n",
    "        text += ' ' + u['activities']\n",
    "        text += ' ' + u['interests']\n",
    "        text += ' ' + u['music']\n",
    "        text += ' ' + u['movies']\n",
    "        text += ' ' + u['tv']\n",
    "        text += ' ' + u['books']\n",
    "        \n",
    "    text += ' ' + users_to_posts[ids_indices_dict[uid]]\n",
    "    \n",
    "    return stemming(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.ir_project\n",
    "ids = create_or_load_ids_dict(db)\n",
    "users_to_posts = create_or_load_users_to_posts(db, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_or_create_texts():\n",
    "    try:\n",
    "        with open('users_texts.bin', 'rb') as in_file:\n",
    "            return pickle.load(in_file)\n",
    "    except:\n",
    "        links_content = links_content_map()\n",
    "        user_links = user_links_map()\n",
    "        \n",
    "        print('Total links parsed: {}'.format(len(links_content)))\n",
    "        \n",
    "        users_texts = dict()\n",
    "\n",
    "        for u in tqdm.tqdm_notebook(db.users.find(no_cursor_timeout=True), total=db.users.count()):\n",
    "            users_texts[u['uid']] = process_user(db, u['uid'], users_to_posts, ids, links_content, user_links)\n",
    "            \n",
    "        with open('users_texts.bin', 'wb') as out:\n",
    "            pickle.dump(users_texts, out)\n",
    "            \n",
    "        return users_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links parsed: 62124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a719fe15df4f53897cab4be9667486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64114), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_texts = read_or_create_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_texts = {uid:text for uid, text in users_texts.items() if len(text) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_texts = [t for t in users_texts.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_WORDS = 10\n",
    "TOPICS_COUNT = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(min_df = 7, max_df = 0.9, stop_words=stops)\n",
    "tf = tf_vectorizer.fit_transform(only_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43988, 103106)\n"
     ]
    }
   ],
   "source": [
    "print(tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladka/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 40\n",
      "iteration: 2 of max_iter: 40\n",
      "iteration: 3 of max_iter: 40\n",
      "iteration: 4 of max_iter: 40\n",
      "iteration: 5 of max_iter: 40\n",
      "iteration: 6 of max_iter: 40\n",
      "iteration: 7 of max_iter: 40\n",
      "iteration: 8 of max_iter: 40\n",
      "iteration: 9 of max_iter: 40\n",
      "iteration: 10 of max_iter: 40\n",
      "iteration: 11 of max_iter: 40\n",
      "iteration: 12 of max_iter: 40\n",
      "iteration: 13 of max_iter: 40\n",
      "iteration: 14 of max_iter: 40\n",
      "iteration: 15 of max_iter: 40\n",
      "iteration: 16 of max_iter: 40\n",
      "iteration: 17 of max_iter: 40\n",
      "iteration: 18 of max_iter: 40\n",
      "iteration: 19 of max_iter: 40\n",
      "iteration: 20 of max_iter: 40\n",
      "iteration: 21 of max_iter: 40\n",
      "iteration: 22 of max_iter: 40\n",
      "iteration: 23 of max_iter: 40\n",
      "iteration: 24 of max_iter: 40\n",
      "iteration: 25 of max_iter: 40\n",
      "iteration: 26 of max_iter: 40\n",
      "iteration: 27 of max_iter: 40\n",
      "iteration: 28 of max_iter: 40\n",
      "iteration: 29 of max_iter: 40\n",
      "iteration: 30 of max_iter: 40\n",
      "iteration: 31 of max_iter: 40\n",
      "iteration: 32 of max_iter: 40\n",
      "iteration: 33 of max_iter: 40\n",
      "iteration: 34 of max_iter: 40\n",
      "iteration: 35 of max_iter: 40\n",
      "iteration: 36 of max_iter: 40\n",
      "iteration: 37 of max_iter: 40\n",
      "iteration: 38 of max_iter: 40\n",
      "iteration: 39 of max_iter: 40\n",
      "iteration: 40 of max_iter: 40\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=TOPICS_COUNT, max_iter=40,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0, verbose=1, n_jobs=1).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "репост сделать запись конкурс группа победитель место друг розыгрыш свой\n",
      "Topic #1:\n",
      "50рубль рубль 40рубль 100рубль 300рубль 200рубль 30рубль 120рубль 250рубль 10рубль\n",
      "Topic #2:\n",
      "дом автомобиль квартира ремонт машина метр комната дверь система материал\n",
      "Topic #3:\n",
      "любить свой друг хороший самый любовь знать женщина мужчина большой\n",
      "Topic #4:\n",
      "святой секс украина господь бог молиться ангел ти мина ми\n",
      "Topic #5:\n",
      "игра gt получить уровень набрать бонус заходить открытка очки новый\n",
      "Topic #6:\n",
      "аниме дрова сорняк пётр story двор pikabu идти кун маркер\n",
      "Topic #7:\n",
      "мир русский война россия стать свой страна история земля век\n",
      "Topic #8:\n",
      "фильм жанр сша история хороший дом любовь самый комедия реж\n",
      "Topic #9:\n",
      "рубль цена купить магазин cc заказ товар скидка доставка сайт\n",
      "Topic #10:\n",
      "ряд плата лицо петлить сантиметр вязать петля лицевой спица сторона\n",
      "Topic #11:\n",
      "язык слово английский буква урок ctrl страница текст alt извинить\n",
      "Topic #12:\n",
      "работа рубль свой компания сайт проект работать россия деньга бизнес\n",
      "Topic #13:\n",
      "масло литр соль добавить яйцо минута ст ингредиент приготовление перец\n",
      "Topic #14:\n",
      "цвета красный цветок цвет зелёный чёрный белый жёлтый роза новый\n",
      "Topic #15:\n",
      "свой самый должный делать большой ребёнок хороший сделать дело отношение\n",
      "Topic #16:\n",
      "музей петербург место адрес город улица ул дом проспект далее\n",
      "Topic #17:\n",
      "fuck маша аромат родинка visitsuomi парфюм месси парфюмерный van окталия\n",
      "Topic #18:\n",
      "москва билет концерт клуб лето любимый праздник питер хороший ждать\n",
      "Topic #19:\n",
      "youtube видео музыка watch альбом канал смотреть music list песня\n",
      "Topic #20:\n",
      "нe нa чтo этo кaк тo мeнить пo oт любить\n",
      "Topic #21:\n",
      "книга роман учебник рассказ александр писатель андрей мир де михаил\n",
      "Topic #22:\n",
      "co love moscow life day one like go time fashion\n",
      "Topic #23:\n",
      "вода кожа нога волос рука растение средство организм упражнение хороший\n",
      "Topic #24:\n",
      "ребёнок собака дом помочь помощь мама малыш просить найти кошка\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, TOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vect.bin', 'wb') as out:\n",
    "    pickle.dump(tf_vectorizer, out)\n",
    "with open('lda.bin', 'wb') as out:\n",
    "    pickle.dump(lda, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_tsne = TSNE(n_components=2).fit_transform(topics)\n",
    "topics_pca = PCA(n_components=2).fit_transform(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
